ğŸ¬ Rotten Tomatoes Movie Metadata Pipeline

Course: Data Collection & Preparation
Project: End-to-End Data Pipeline â€” Dynamic Website â†’ Cleaning â†’ SQLite â†’ Airflow Automation

ğŸ“Œ 1. Overview

This project implements a fully automated data pipeline that collects, cleans, and stores movie metadata from Rotten Tomatoes, a highly dynamic website rendered with JavaScript. The system integrates:

Playwright (Chromium) for dynamic scraping

Python + Pandas for preprocessing

SQLite for persistent storage

Apache Airflow for orchestration

asyncio for concurrency

Logging & retries for reliability

The workflow follows:

Scrape movie list â†’ Scrape metadata â†’ Clean â†’ Load into SQLite â†’ Automate with Airflow


All assignment requirements (dynamic scraping, cleaning, automation, DB storage) are fully met.

ğŸŒ 2. Website Description

Target URL:
https://www.rottentomatoes.com/browse/movies_at_home

Why this website qualifies:

Uses React components

Metadata and scorecards load dynamically

Data not present in raw HTML

Requires browser execution to access elements

Playwright Chromium is therefore used in headless mode to load and parse 8 paginated pages (?page=0..7).

ğŸ§± 3. Pipeline Architecture
```txt
Airflow DAG
â”‚
â”œâ”€â”€ Task 1: scrape_movie_list
â”‚   â€¢ Scrapes 8 dynamic pages
â”‚   â€¢ Extracts titles + URLs
â”‚   â€¢ Saves movies_raw.csv
â”‚
â”œâ”€â”€ Task 2: scrape_movie_details
â”‚   â€¢ Opens each movie page with Playwright
â”‚   â€¢ Extracts all metadata fields
â”‚   â€¢ Cleans + validates data
â”‚   â€¢ Saves movies_clean.csv
â”‚
â””â”€â”€ Task 3: load_to_sqlite
    â€¢ Inserts cleaned data into SQLite
    â€¢ Creates indexes
    â€¢ Verifies successful load
```




ğŸ›  4. Technology Stack
Component	Tool
Dynamic scraping	Playwright (Chromium)
Concurrency	asyncio + semaphores
Cleaning	Python, Pandas, regex
Storage	SQLite3
Scheduler	Apache Airflow
Deployment	Docker Compose
Logging	Python logging
ğŸ¥ 5. Data Collection (Scraper)
Tool Used: Playwright

(Assignment requirement: dynamic JS scraping â†’ fulfilled)

Playwright handles:

JavaScript execution

Interactive DOM elements

Lazy-loaded score components

Navigation through pages

Extracting structured data via CSS selectors

Extracted Data
From list pages:

Movie title

Detail page URL

From movie detail pages:

Tomatometer score

Audience score

Genre

Rating

Duration

Release date

Director

Original language

Box office

Distributor

All scraping is asynchronous and batched using semaphores for performance.

ğŸ§¹ 6. Data Cleaning & Normalization

The cleaning system performs:

âœ” Duplicate Removal

URL normalization

Title normalization with regex

Removal of repeated titles or URLs

âœ” Handling Missing Data

Missing values stay as None â†’ become NULL in SQLite

âœ” Text Normalization

Lowercasing

Whitespace cleanup

Punctuation removal

Duration converted to 1h 42m format

Release date cleanup

âœ” Type Casting & Validation

Score fields must be 0â€“100

Invalid values removed

Columns cast to correct types

Output file:

movies_clean.csv

ğŸ—„ 7. SQLite Database Layer

Database file location:
/opt/airflow/data/movies.db

Table Schema
Column	Type
title	TEXT
url	TEXT
tomatometer_score	INTEGER
audience_score	INTEGER
genre	TEXT
rating	TEXT
duration	TEXT
release_date	TEXT
director	TEXT
original_language	TEXT
box_office	TEXT
distributor	TEXT


ğŸª‚ 8. Airflow Automation

DAG ID: scraper
Schedule: @daily (meets assignment requirement: no more than once every 24 hours)

Tasks
1. scrape_movie_list

Scrapes 8 dynamic pages

Saves movies_raw.csv

2. scrape_movie_details

Opens each movie page

Extracts + cleans metadata

Saves movies_clean.csv

3. load_to_sqlite

Loads cleaned data

Creates indexes

Confirms row counts

Resilience Features

2 retries

5 minute retry delay

90 minute timeout

Logging for every step

Progress metrics in Airflow logs

â–¶ï¸ 9. Running the Project
Install dependencies
pip install -r requirements.txt
playwright install chromium

Start Airflow services
airflow db init
airflow webserver -p 8080
airflow scheduler

Enable the DAG

Airflow UI â†’ turn on scraper

Expected Outputs

movies_raw.csv

movies_clean.csv

movies.db

Example Cleaned Row
Field	Example
title	Zootopia
tomatometer_score	98
audience_score	92
genre	Animation, Comedy
duration	1h 48m
director	Byron Howard, Rich Moore

ğŸ“ 10. Project Structure
```txt

AIRFLOW/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ project.py               # Airflow DAG: scraping + cleaning + loading
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ movies_raw.csv           # Output: scraped movie list (Task 1)
â”‚   â”œâ”€â”€ movies_clean.csv         # Output: cleaned metadata (Task 2)
â”‚   â””â”€â”€ movies.db                # SQLite database (Task 3)
â”‚
â”œâ”€â”€ logs/                        # Airflow execution logs
â”‚
â”œâ”€â”€ plugins/                    
â”‚
â”œâ”€â”€ config/                     
â”‚
â”œâ”€â”€ .env                         # Environment variables (Playwright / Airflow)
â”‚
â”œâ”€â”€ docker-compose.yaml          # Airflow Docker Compose stack
â”‚
â”œâ”€â”€ Dockerfile                   # Custom Dockerfile (Playwright + dependencies)
â”‚
â””â”€â”€ requirements.txt             # Python dependencies (Playwright, Pandas, etc.)
```
